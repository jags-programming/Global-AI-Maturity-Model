# GAIMM Diagnostic Questionnaires

This document includes pillar-wise diagnostic questions to assess maturity across GAIMM levels. Each question is aligned with the evaluation criteria already defined for the respective level.

---

## Pillar 1: Strategy & Governance

### Level 1: Exploratory

* Is AI adoption driven by isolated experimentation without central oversight?
* Are there any formalized AI governance structures or policies in place?
* Are AI use cases initiated without business alignment?
* Do stakeholders lack a shared understanding of AI potential?
* Is there no budget or strategy defined for AI initiatives?

### Level 2: Foundational

* Is there an initial AI strategy aligned with IT or innovation departments?
* Are basic governance mechanisms or review boards introduced?
* Do you have emerging frameworks for aligning AI efforts with business outcomes?
* Are stakeholders aware of AI opportunities and risks?
* Is there any allocation of resources or investment toward AI?

### Level 3: Structured

* Is there a well-defined, organization-wide AI strategy in place?
* Are centralized governance structures and decision rights established?
* Are AI initiatives assessed for business value and risk before execution?
* Are roadmaps developed for scaling AI across functions?
* Is AI strategy periodically reviewed and refined?

### Level 4: Enterprise-Integrated

* Are AI goals embedded in broader enterprise KPIs and objectives?
* Is the AI governance board functioning cross-functionally?
* Are AI outcomes continuously measured against business performance?
* Is there funding for enterprise AI programs with strategic impact?
* Is the AI strategy driving changes in business models or processes?

### Level 5: Transformative

* Is AI a board-level priority integrated into enterprise growth strategy?
* Are governance models agile and adaptive to innovation cycles?
* Is AI central to long-term competitive differentiation?
* Is the enterprise recognized externally for AI maturity?
* Are AI initiatives continuously optimized for value realization?

---

## Pillar 2: Organization & Talent

### Level 1: Exploratory

* Are there no defined roles for AI or data science within the organization?
* Do current employees lack AI-specific training or upskilling opportunities?
* Are AI efforts pursued by isolated individuals or teams?
* Is there limited awareness of the importance of AI competencies?
* Are AI responsibilities added informally to existing roles?

### Level 2: Foundational

* Have emerging AI roles (e.g., data scientist, ML engineer) been introduced?
* Are basic training or awareness sessions on AI being offered?
* Is there recognition of the need for AI talent development?
* Are initial efforts made to recruit AI or data talent?
* Is there a focal point or champion for AI in the organization?

### Level 3: Structured

* Are there well-defined career paths and competency models for AI roles?
* Are structured upskilling or reskilling programs in place?
* Has the organization established a formal AI Center of Excellence (CoE)?
* Are AI teams embedded within business functions?
* Are AI roles incorporated into the organization’s workforce planning?

### Level 4: Enterprise-Integrated

* Is there an enterprise-wide AI talent strategy aligned with business needs?
* Are AI capabilities available across departments and geographies?
* Are cross-functional AI teams working collaboratively?
* Is AI embedded into leadership development and L\&D programs?
* Are internal communities of practice supporting AI skill development?

### Level 5: Transformative

* Is the organization continuously evolving its AI talent strategy?
* Are internal AI experts recognized as thought leaders?
* Are AI teams empowered to drive innovation and organizational change?
* Is the enterprise actively shaping external AI talent ecosystems?
* Are advanced AI skillsets considered core organizational competencies?

---

## Pillar 3: Data & Infrastructure

### Level 1: Exploratory

* Are AI projects using ad-hoc, siloed datasets?
* Is data infrastructure fragmented or lacking scalability?
* Are there any enterprise data governance standards in place?
* Do teams face difficulty accessing clean, labeled, and reliable data?
* Are AI infrastructure tools selected without coordination?

### Level 2: Foundational

* Have data lakes or warehouses begun consolidating key data sources?
* Are early data governance policies and metadata standards emerging?
* Are initial MLOps or DevOps tools being used for experimentation?
* Do teams have some reusable data pipelines for AI tasks?
* Is cloud or on-prem AI infrastructure provisioning basic but operational?

### Level 3: Structured

* Is data infrastructure standardized across the enterprise?
* Are scalable AI platforms or cloud-native services widely adopted?
* Are MLOps pipelines in place for model development and deployment?
* Is access to high-quality training data reliable and repeatable?
* Are monitoring and lineage tools tracking data and models?

### Level 4: Enterprise-Integrated

* Is infrastructure dynamically provisioning for enterprise AI needs?
* Are common data platforms and APIs supporting cross-functional reuse?
* Are AI workloads orchestrated across hybrid or multi-cloud setups?
* Is there enterprise-wide governance over data quality and access?
* Are data and infrastructure considered a strategic enabler for AI?

### Level 5: Transformative

* Is the AI infrastructure enabling real-time, autonomous systems?
* Are self-healing, intelligent MLOps pipelines operational?
* Are unified data fabrics or knowledge graphs driving AI integration?
* Is infrastructure continually optimized based on workload patterns?
* Is the enterprise a reference case for scalable AI infrastructure?

---

## Pillar 4: AI Lifecycle & Operations

### Level 1: Exploratory

* Are AI models developed on an ad-hoc basis without versioning or standard practices?
* Is model experimentation untracked or inconsistent?
* Are deployment processes manual and siloed?
* Is feedback from AI outcomes rarely captured or analyzed?
* Are AI systems not monitored post-deployment?

### Level 2: Foundational

* Have basic processes for model development and retraining been introduced?
* Are deployment pipelines being piloted for select models?
* Is model performance tracking partially in place?
* Are some feedback mechanisms used to update models?
* Are there templates or documentation emerging for AI lifecycle management?

### Level 3: Structured

* Are AI lifecycle workflows standardized across the organization?
* Are there CI/CD pipelines for model testing and deployment?
* Is feedback from end users or systems integrated into model improvement?
* Is monitoring of fairness, accuracy, and drift ongoing?
* Are model repositories, versioning, and reuse well managed?

### Level 4: Enterprise-Integrated

* Are AI operations integrated into broader IT and DevOps processes?
* Are models retrained systematically based on data and usage changes?
* Are governance policies enforced throughout the model lifecycle?
* Are lifecycle tools embedded within business systems?
* Are deployment cycles predictable, scalable, and aligned with SLAs?

### Level 5: Transformative

* Are AI systems self-adaptive and continuously learning from new data?
* Is there real-time lifecycle management for AI assets?
* Are lifecycle metrics tied to business KPIs?
* Are external benchmarks used to optimize model performance?
* Is the enterprise leading in best practices for AI lifecycle operations?

---

## Pillar 5: Ethics, Risk & Compliance

### Level 1: Exploratory

* Are ethical and compliance risks of AI not yet considered or documented?
* Is there no dedicated ownership or accountability for AI risk management?
* Are AI systems developed without fairness or transparency assessments?
* Are risks identified only after deployment or incidents?
* Is bias in models neither measured nor mitigated?

### Level 2: Foundational

* Are early policies or codes of conduct emerging for responsible AI?
* Are privacy, security, and fairness addressed in select projects?
* Are there basic reviews of model risk before deployment?
* Are early efforts made to document model decisions or logic?
* Are roles for ethical oversight being considered or defined?

### Level 3: Structured

* Are there formal governance processes for AI risk and compliance?
* Are impact assessments (e.g., for bias, fairness, explainability) embedded in workflows?
* Are model audits or validations conducted periodically?
* Are AI ethics frameworks trained across departments?
* Are stakeholder concerns addressed via formal channels?

### Level 4: Enterprise-Integrated

* Is AI ethics embedded into product lifecycle and business policies?
* Are risk controls automated and enforced across the AI lifecycle?
* Are independent audits or third-party assessments in place?
* Are risk registers maintained for all critical AI applications?
* Are compliance and legal functions actively involved in AI delivery?

### Level 5: Transformative

* Is the enterprise recognized as a leader in responsible AI?
* Are ethics-by-design principles driving AI innovation?
* Are continuous monitoring and risk mitigation integrated into AI systems?
* Is responsible AI a competitive differentiator and brand value?
* Are ethical risks managed proactively using real-time dashboards and alerts?

---

### Pillar 6: Impact & Value Realization – Diagnostic Questions

**Level 1: Exploratory**

1. Are AI projects pursued primarily as isolated experiments or pilots?
2. Is there any formal tracking of business outcomes from AI use cases?
3. Are success stories anecdotal without structured benefit reporting?
4. Is AI value realization discussed at strategic leadership levels?
5. Are scalability or replicability of AI initiatives considered during development?

**Level 2: Foundational**

1. Are there efforts underway to identify and prioritize potential AI use cases?
2. Is there a defined framework for measuring ROI or value from AI?
3. Are some use cases showing early signs of business impact?
4. Are teams beginning to use performance metrics to assess AI solutions?
5. Are early AI solutions evaluated for future scaling potential?

**Level 3: Structured**

1. Are AI use cases selected based on defined business KPIs or objectives?
2. Is value realization tracked consistently across multiple use cases?
3. Is there a repeatable framework for piloting and scaling AI initiatives?
4. Are there defined roles or teams responsible for benefit realization?
5. Are AI solutions reused or adapted across functions?

**Level 4: Enterprise-Integrated**

1. Are AI initiatives closely aligned with corporate strategy and KPIs?
2. Are value metrics integrated into AI project lifecycles from planning through deployment?
3. Is AI impact quantified and reviewed at the executive level?
4. Are scaled AI solutions delivering measurable productivity or revenue improvements?
5. Are AI-driven insights embedded into day-to-day decision-making across departments?

**Level 5: Transformative**

1. Is AI a core driver of business model innovation and competitiveness?
2. Are learnings from AI impact systematically fed into strategic planning?
3. Are predictive and prescriptive analytics continuously optimizing operations?
4. Is the organization known externally for AI-led value creation?
5. Are there mature feedback loops between AI insights and business growth strategies?


